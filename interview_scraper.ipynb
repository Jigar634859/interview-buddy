{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFsmxqcgIVub"
      },
      "outputs": [],
      "source": [
        "!pip install fpdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQXAJT_0IfMa"
      },
      "outputs": [],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kksDYNpzqIL6"
      },
      "source": [
        "üîç Web Scraper for GeeksforGeeks Interview Experiences\n",
        "This script scrapes Adobe interview experiences from the GeeksforGeeks Experienced Interviews page. It performs the following:\n",
        "\n",
        "1.Locates the section for the specified company (Adobe).\n",
        "\n",
        "2.Extracts all interview links under that section.\n",
        "\n",
        "3.Infers the candidate‚Äôs years of experience and maps it to a role (SDE-1, SDE-2, SDE-3).\n",
        "\n",
        "4.Saves the data (Title, Link, Years, Role) into a CSV file named adobe_experiences.csv.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQfxGYvsHvn2"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup, NavigableString, Tag\n",
        "import re\n",
        "import csv\n",
        "\n",
        "BASE_URL = \"https://www.geeksforgeeks.org/interview-experiences/experienced-interview-experiences-company-wise/\"\n",
        "COMPANY = \"Adobe\"\n",
        "\n",
        "def infer_role_and_years(title):\n",
        "    m = re.search(r'(\\d+(\\.\\d+)?)\\s*(?:yr|year)', title, re.IGNORECASE)\n",
        "    yrs = float(m.group(1)) if m else 0.0\n",
        "    if yrs <= 2:\n",
        "        role = \"SDE-1\"\n",
        "    elif yrs <= 5:\n",
        "        role = \"SDE-2\"\n",
        "    else:\n",
        "        role = \"SDE-3\"\n",
        "    return yrs, role\n",
        "\n",
        "def scrape_amazon_experiences():\n",
        "    resp = requests.get(BASE_URL)\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "    # 1) Locate the exact text node \"Amazon :\"\n",
        "    amazon_node = soup.find(string=re.compile(r'^\\s*Adobe\\s*:$'))\n",
        "    if not amazon_node:\n",
        "        print(\"‚ùå Could not locate the text node ‚ÄúAmazon :‚Äù.\")\n",
        "        return []\n",
        "\n",
        "    entries = []\n",
        "    # 2) Iterate through all following elements until the next \"Label :\"\n",
        "    for elem in amazon_node.next_elements:\n",
        "        # If we hit another label like \"Aphonso :\", we stop\n",
        "        if isinstance(elem, NavigableString) and re.match(r'^\\s*[A-Za-z0-9 &]+\\s*:$', elem.strip()) \\\n",
        "           and elem.strip().lower() != \"amazon:\":\n",
        "            break\n",
        "\n",
        "        # Whenever we see an <a href=\"...\"> that's an interview link, grab it\n",
        "        if isinstance(elem, Tag) and elem.name == \"a\" and elem.get(\"href\"):\n",
        "            title = elem.get_text(strip=True)\n",
        "            link  = elem[\"href\"]\n",
        "            yrs, role = infer_role_and_years(title)\n",
        "            entries.append({\n",
        "                \"Company\": COMPANY,\n",
        "                \"Title\": title,\n",
        "                \"Link\": link,\n",
        "                \"Years\": yrs,\n",
        "                \"Role\": role\n",
        "            })\n",
        "\n",
        "    return entries\n",
        "\n",
        "def main():\n",
        "    data = scrape_amazon_experiences()\n",
        "    print(f\"‚úÖ Found {len(data)} entries for {COMPANY}.\")  # expect around 61\n",
        "\n",
        "    if not data:\n",
        "        return\n",
        "\n",
        "    # Write to CSV\n",
        "    fname = f\"{COMPANY.lower()}_experiences.csv\"\n",
        "    with open(fname, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"Company\",\"Title\",\"Link\",\"Years\",\"Role\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "    print(f\"‚úÖ Written details to {fname}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTHD8F9jq8eF"
      },
      "source": [
        "Here‚Äôs a concise summary for the second script, suitable for a Google Colab text cell:\n",
        "\n",
        "---\n",
        "\n",
        "### üìÑ Extract Full Interview Experience Content from GFG Links\n",
        "\n",
        "This script reads a list of Adobe interview experience links from `adobe_experiences.csv`, fetches each webpage, and extracts structured content:\n",
        "\n",
        "* Uses `BeautifulSoup` to parse the interview article page.\n",
        "* Identifies interview rounds based on `<strong>` tags (like ‚ÄúRound 1‚Äù, ‚ÄúTechnical Round‚Äù, etc.).\n",
        "* Preserves round titles and HTML formatting for better structure.\n",
        "* Handles inconsistent HTML and adds fallback logic for different page layouts.\n",
        "* Appends the extracted experience text to each row and writes the output to `adobe_experiences_full_text.csv`.\n",
        "* Adds error handling and polite scraping via delays between requests.\n",
        "\n",
        "At the end, it shows a preview of the first successfully extracted interview content.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljygP3KUJ87-"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import re\n",
        "import time\n",
        "\n",
        "def fetch_full_text(link):\n",
        "\n",
        "    try:\n",
        "        # Add headers to avoid blocking\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        resp = requests.get(link, headers=headers, timeout=30)\n",
        "        resp.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "        # Find the main content div\n",
        "        text_div = soup.find(\"div\", class_=\"text\")\n",
        "        if not text_div:\n",
        "            # Fallback options for different page structures\n",
        "            text_div = (soup.find(\"div\", class_=\"entry-content\") or\n",
        "                       soup.find(\"article\") or\n",
        "                       soup.find(\"div\", class_=\"content\") or\n",
        "                       soup.body)\n",
        "\n",
        "        if not text_div:\n",
        "            return \"Content div not found\"\n",
        "\n",
        "        full_experience = []\n",
        "\n",
        "        # Find all <strong> tags that likely mark interview rounds\n",
        "        strong_tags = text_div.find_all('strong')\n",
        "\n",
        "        if not strong_tags:\n",
        "            # If no strong tags, return the entire text content (cleaned)\n",
        "            clean_text = text_div.get_text(separator=' ', strip=True)\n",
        "            # Remove extra whitespace\n",
        "            clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
        "            return clean_text\n",
        "\n",
        "        # Process each round\n",
        "        for i, strong in enumerate(strong_tags):\n",
        "            round_title = strong.get_text(strip=True)\n",
        "\n",
        "            # Enhanced round detection - look for round indicators\n",
        "            round_keywords = ['round', 'interview', 'telephonic', 'f2f', 'phone', 'onsite',\n",
        "                            'technical', 'hr', 'managerial', 'written', 'coding', 'design',\n",
        "                            'screening', 'assessment', 'test']\n",
        "\n",
        "            # Skip if it doesn't look like a round\n",
        "            if not any(keyword in round_title.lower() for keyword in round_keywords):\n",
        "                # But include if it looks like \"Round 1\", \"Round 2\", etc.\n",
        "                if not re.match(r'.*round\\s*\\d+', round_title.lower()):\n",
        "                    continue\n",
        "\n",
        "            # Collect content until next strong tag or end\n",
        "            content_parts = []\n",
        "            current = strong.next_sibling\n",
        "\n",
        "            while current:\n",
        "                # Stop if we hit another round\n",
        "                if current.name == 'strong':\n",
        "                    next_strong_text = current.get_text(strip=True)\n",
        "                    if any(keyword in next_strong_text.lower() for keyword in round_keywords):\n",
        "                        break\n",
        "                    # Also break on \"Round X\" patterns\n",
        "                    if re.match(r'.*round\\s*\\d+', next_strong_text.lower()):\n",
        "                        break\n",
        "\n",
        "                if isinstance(current, str):\n",
        "                    content_parts.append(current)\n",
        "                else:\n",
        "                    # Preserve HTML structure but clean it up\n",
        "                    content_parts.append(str(current))\n",
        "\n",
        "                current = current.next_sibling\n",
        "\n",
        "            # Clean and format the content\n",
        "            round_content = ''.join(content_parts).strip()\n",
        "\n",
        "            if round_content:\n",
        "                # Clean up extra whitespace and remove comments\n",
        "                round_content = re.sub(r'\\s+', ' ', round_content)\n",
        "                round_content = re.sub(r'<!--.*?-->', '', round_content, flags=re.DOTALL)\n",
        "                # Clean up HTML artifacts\n",
        "                round_content = re.sub(r'</?div[^>]*>', '', round_content)\n",
        "                round_content = round_content.strip()\n",
        "\n",
        "                if round_content:  # Only add if there's actual content\n",
        "                    full_experience.append(f\"<h3>{round_title}</h3>\\n{round_content}\\n\")\n",
        "\n",
        "        result = '\\n'.join(full_experience) if full_experience else text_div.get_text(separator=' ', strip=True)\n",
        "\n",
        "        # Final cleanup\n",
        "        result = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', result)  # Remove excessive newlines\n",
        "        return result.strip()\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Network error: {str(e)}\"\n",
        "    except Exception as e:\n",
        "        return f\"Parsing error: {str(e)}\"\n",
        "\n",
        "def main():\n",
        "    INPUT_CSV = \"adobe_experiences.csv\"\n",
        "    OUTPUT_CSV = \"adobe_experiences_full_text.csv\"\n",
        "\n",
        "    rows = []\n",
        "    processed_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    with open(INPUT_CSV, newline='', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        fieldnames = list(reader.fieldnames) + [\"Interview_Experience\"]\n",
        "\n",
        "        for i, entry in enumerate(reader):\n",
        "            link = entry.get('Link', '')\n",
        "            title = entry.get('Title', '')\n",
        "\n",
        "            print(f\"Fetching {i+1}: {title[:50]}...\")\n",
        "\n",
        "            # Add delay to be respectful to the server\n",
        "            if i > 0:\n",
        "                time.sleep(2)  # 2 second delay between requests\n",
        "\n",
        "            try:\n",
        "                interview_content = fetch_full_text(link)\n",
        "                if interview_content.startswith(('Network error:', 'Parsing error:')):\n",
        "                    error_count += 1\n",
        "                else:\n",
        "                    processed_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Unexpected error processing {link}: {e}\")\n",
        "                interview_content = f\"Unexpected error: {str(e)}\"\n",
        "                error_count += 1\n",
        "\n",
        "            rows.append({**entry, 'Interview_Experience': interview_content})\n",
        "\n",
        "    # Write to output CSV\n",
        "    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "\n",
        "    print(f\"\\n‚úÖ Processing complete!\")\n",
        "    print(f\"   Total entries: {len(rows)}\")\n",
        "    print(f\"   Successfully processed: {processed_count}\")\n",
        "    print(f\"   Errors: {error_count}\")\n",
        "    print(f\"   Output file: {OUTPUT_CSV}\")\n",
        "\n",
        "    # Show sample of what was extracted\n",
        "    if rows:\n",
        "        print(f\"\\nSample from first successful entry:\")\n",
        "        for row in rows:\n",
        "            content = row.get('Interview_Experience', '')\n",
        "            if not content.startswith(('Network error:', 'Parsing error:', 'Unexpected error:')):\n",
        "                print(f\"Title: {row.get('Title', 'N/A')}\")\n",
        "                print(f\"Content preview (first 400 chars):\")\n",
        "                print(f\"{content[:400]}...\")\n",
        "                break\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwRun1Utrup1"
      },
      "source": [
        "## Summary for Google Colab Text Cell\n",
        "\n",
        "- Loads interview data from a CSV file using pandas.\n",
        "- Filters rows where the 'Role' column is exactly 'SDE-1' (case-insensitive, trims spaces).\n",
        "- Saves filtered results to a new CSV file named `sde_one_only.csv`.\n",
        "- Prints the count of SDE-1 entries and shows a preview of the filtered data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiXn-cPzKP5U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = \"adobe_experiences_full_text.csv\"  # Replace with the path to your CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Normalize and filter for Role == 'SDE-1'\n",
        "filtered_df = df[df['Role'].str.strip().str.upper() == 'SDE-1']\n",
        "\n",
        "# Save the filtered result to a new CSV file (optional)\n",
        "filtered_df.to_csv(\"sde_one_only.csv\", index=False)\n",
        "\n",
        "# Print number of filtered rows and preview\n",
        "print(f\"Total entries with Role 'SDE-1': {len(filtered_df)}\")\n",
        "print(filtered_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NqP61Eztsjg"
      },
      "source": [
        "- The script processes SDE-1 interview experiences from a CSV, summarizes each using the Groq API, and saves the results.\n",
        "- It uses the **gemma2-9b-it** model for summarization because this model is optimized for instruction-following tasks, making it suitable for extracting structured, concise interview summaries from complex HTML content.\n",
        "- The approach ensures coding questions with links are preserved in markdown format, and all key interview aspects (technical, coding, design, behavioral, structure) are covered.\n",
        "- A 1.2-second delay is added between API calls for rate limiting.\n",
        "- Summaries are saved to a new CSV file for further analysis or review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwyubeuIKgna"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "client = Groq(api_key=\"api-key\")\n",
        "\n",
        "def summarize_interview_experience(html_content):\n",
        "\n",
        "    if pd.isnull(html_content):\n",
        "        return \"\"\n",
        "\n",
        "    # Keep both raw HTML (for links) and clean text\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    clean_text = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are given a software-engineering interview write-up in HTML. Extract and summarize it in 3‚Äì5 bullet points, covering:\n",
        "- Technical questions asked\n",
        "- Coding challenges (further divide these into topics: Array, Tree, String, DP, Graph)\n",
        "- System-design components\n",
        "- Behavioral questions\n",
        "- Interview structure/rounds\n",
        "\n",
        "**Important:** For every coding question, if the original HTML contained an `<a>` anchor, include it in markdown form `[Question text](URL)`.\n",
        "\n",
        "---\n",
        "**Interview write-up (plain text):**\n",
        "{clean_text[:3000]}\n",
        "\n",
        "---\n",
        "**Interview write-up (raw HTML, for link extraction):**\n",
        "{html_content[:3000]}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gemma2-9b-it\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.2,\n",
        "            max_completion_tokens=400\n",
        "        )\n",
        "        return completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error summarizing: {e}\")\n",
        "        return \"\"\n",
        "# Load your data\n",
        "df = pd.read_csv('sde_one_only.csv')\n",
        "\n",
        "# Process interviews with rate limiting\n",
        "summaries = []\n",
        "for idx, row in df.iterrows():\n",
        "    print(f\"Processing row {idx+1}/{len(df)}\")\n",
        "    summary = summarize_interview_experience(row['Interview_Experience'])\n",
        "    summaries.append(summary)\n",
        "    time.sleep(1.2)  # Maintain 1.2s between requests\n",
        "\n",
        "# Save results\n",
        "df['Interview_Summary'] = summaries\n",
        "df.to_csv('summarize_interviews.csv', index=False)\n",
        "print(\"Saved summarize_interviews.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fZb4Fb3uNnH"
      },
      "source": [
        "This script aggregates and processes summarized SDE-1 interview data to create a structured PDF-style report. It uses the **deepseek-r1-distill-llama-70b** model through Groq's API, chosen for its capacity to handle complex technical content and generate well-structured markdown outputs required for professional documentation. Key aspects:\n",
        "\n",
        "**Model Selection Rationale**  \n",
        "- Optimized for technical content synthesis and instruction-following  \n",
        "- Balances conciseness with detail retention for multi-section reports  \n",
        "- Handles markdown formatting requirements natively  \n",
        "\n",
        "**Workflow**  \n",
        "1. Merges individual interview summaries into a single text corpus  \n",
        "2. Uses LLM to extract patterns and organize content into:  \n",
        "   - Technical topics (Array, Tree, String, DP, Graph)  \n",
        "   - Behavioral questions with sample answers  \n",
        "   - System design components  \n",
        "   - Interview structure analysis  \n",
        "3. Preserves coding challenge links in markdown format  \n",
        "4. Exports both full report and question-link pairs as CSVs  \n",
        "\n",
        "The temperature setting (0.2) ensures factual consistency while allowing some variability in advice formulation. This builds on previous data processing work with pandas and aligns with the user's goal of creating structured interview preparation materials.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v78S7meCK4EV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from groq import Groq\n",
        "\n",
        "# Initialize Groq client with API key as a string\n",
        "client = Groq(api_key=\"api-key\")\n",
        "\n",
        "# Load your data and merge summaries\n",
        "df = pd.read_csv('summarize_interviews.csv')\n",
        "merged_summary = '\\n'.join(df['Interview_Summary'].astype(str).tolist())\n",
        "\n",
        "# Enhanced prompt for SDE-1 interview PDF-style summary including request for markdown links\n",
        "prompt = f\"\"\"\n",
        "You are an expert technical interviewer and career coach.\n",
        "\n",
        "Given a merged summary of several software engineering interview experiences, generate a detailed, well-structured summary report similar to a professional SDE-1 interview preparation PDF.\n",
        "\n",
        "The summary should include:\n",
        "- **Key technical topics** covered (Array, Tree, String, DP, Graph, System Design)\n",
        "- **Common behavioral questions** and sample answers\n",
        "- **Coding challenges** with examples and links if present (keep markdown links as `[Question text](URL)`)\n",
        "- **System design questions** and key points\n",
        "- **Interview structure and rounds** overview\n",
        "- **Notable patterns, advice, and tips** for candidates\n",
        "\n",
        "Format the output as markdown with clear headings, bullet points, and tables where appropriate. Ensure it is concise, professional, and shloud in pdf form.\n",
        "\n",
        "---\n",
        "Merged Interview Summaries:\n",
        "{merged_summary[:6000]}\n",
        "---\n",
        "\"\"\"\n",
        "\n",
        "def summarize_with_llm(text, prompt, max_tokens=1024):\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"deepseek-r1-distill-llama-70b\",  # Or your preferred model\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.2,\n",
        "            max_completion_tokens=max_tokens\n",
        "        )\n",
        "        return completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error summarizing: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Generate the summary\n",
        "final_summary = summarize_with_llm(merged_summary, prompt)\n",
        "\n",
        "# Extract all markdown links [text](url) from the final summary\n",
        "pattern = r'\\[([^\\]]+)\\]\\((https?://[^\\)]+)\\)'\n",
        "matches = re.findall(pattern, final_summary)\n",
        "\n",
        "# Create a DataFrame of questions and links\n",
        "questions_links_df = pd.DataFrame(matches, columns=['Coding Question', 'Link'])\n",
        "\n",
        "# Save the table of questions and links\n",
        "questions_links_df.to_csv('sde1_coding_questions_links.csv', index=False)\n",
        "\n",
        "# Save the full summary as CSV (single cell) for markdown/PDF conversion\n",
        "output_df = pd.DataFrame({'Summary': [final_summary]})\n",
        "output_df.to_csv('sde1_final_summary.csv', index=False)\n",
        "\n",
        "print(\"Final summary and coding question links saved successfully.\")\n",
        "print(final_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWhkXKE8LBX0"
      },
      "outputs": [],
      "source": [
        "!pip install weasyprint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBa5MDQXuvgL"
      },
      "source": [
        "- Loads the SDE-1 interview summary, cleans unwanted tags, and analyzes the frequency of key topics.\n",
        "- Generates a pie chart showing topic distribution and embeds it in the report.\n",
        "- Converts the markdown summary (with clickable links) to styled HTML.\n",
        "- Combines the chart and summary into a polished PDF, preserving all links for easy access.\n",
        "- Final output: a professional SDE-1 interview preparation PDF with visual analytics and structured content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ze7y2f-LDz4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import markdown\n",
        "from weasyprint import HTML\n",
        "import matplotlib.pyplot as plt\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "# Step 1: Load the summary\n",
        "df = pd.read_csv(\"sde1_final_summary.csv\")\n",
        "summary_md = df.iloc[0]['Summary']\n",
        "\n",
        "# Step 2: Remove all <think>...</think> sections (case-insensitive)\n",
        "summary_md_cleaned = re.sub(r'<think>.*?</think>', '', summary_md, flags=re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "# Step 3: Analyze topic distribution\n",
        "topics = ['Array', 'Tree', 'String', 'DP', 'Graph', 'System Design', 'Behavioral Questions']\n",
        "topic_counts = {\n",
        "    topic: len(re.findall(rf'\\b{topic}\\b', summary_md_cleaned, flags=re.IGNORECASE))\n",
        "    for topic in topics\n",
        "}\n",
        "topic_counts = {k: v for k, v in topic_counts.items() if v > 0}\n",
        "\n",
        "# Step 4: Create pie chart as base64 image\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "ax.pie(topic_counts.values(), labels=topic_counts.keys(), autopct='%1.1f%%', startangle=140)\n",
        "ax.set_title(\"Topic Distribution in Interview Summary\")\n",
        "buf = BytesIO()\n",
        "plt.savefig(buf, format='png', bbox_inches='tight')\n",
        "plt.close(fig)\n",
        "buf.seek(0)\n",
        "img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n",
        "img_html = f'<img src=\"data:image/png;base64,{img_base64}\" alt=\"Topic Distribution Chart\" style=\"max-width:100%; height:auto;\">'\n",
        "\n",
        "# Step 5: Convert markdown to HTML with clickable links\n",
        "html_summary = markdown.markdown(\n",
        "    summary_md_cleaned,\n",
        "    extensions=['extra', 'tables', 'sane_lists']\n",
        ")\n",
        "\n",
        "# Step 6: Prepare final HTML with chart + summary\n",
        "styled_html = f\"\"\"\n",
        "<html>\n",
        "<head>\n",
        "    <meta charset=\"utf-8\">\n",
        "    <style>\n",
        "        @page {{\n",
        "            size: A4;\n",
        "            margin: 1in;\n",
        "        }}\n",
        "        body {{\n",
        "            font-family: 'Segoe UI', sans-serif;\n",
        "            line-height: 1.6;\n",
        "            padding: 0;\n",
        "            font-size: 14px;\n",
        "            color: #333;\n",
        "        }}\n",
        "        table {{\n",
        "            border-collapse: collapse;\n",
        "            width: 100%;\n",
        "        }}\n",
        "        table, th, td {{\n",
        "            border: 1px solid #aaa;\n",
        "        }}\n",
        "        th, td {{\n",
        "            padding: 8px;\n",
        "            text-align: left;\n",
        "        }}\n",
        "        h1, h2, h3 {{\n",
        "            color: #2c3e50;\n",
        "        }}\n",
        "        a {{\n",
        "            color: #1e88e5;\n",
        "            text-decoration: underline;\n",
        "        }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>SDE-1 Interview Preparation Summary</h1>\n",
        "    <h2>üìä Topic Distribution</h2>\n",
        "    {img_html}\n",
        "    <hr>\n",
        "    {html_summary}\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Step 7: Generate PDF with clickable links\n",
        "HTML(string=styled_html, base_url='.').write_pdf(\"sde1_summary_report_with_chart.pdf\")\n",
        "\n",
        "print(\"‚úÖ PDF with cleaned summary and clickable links saved as 'sde1_summary_report_with_chart.pdf'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxnzhloqN5a4"
      },
      "outputs": [],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28dlqInt6SBG"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i36r01B5u7eL"
      },
      "source": [
        "This script implements a **synchronized pipeline** that integrates all previous code components into an automated end-to-end solution for generating company-specific SDE interview preparation materials. Key features:\n",
        "\n",
        "**Automated Workflow**  \n",
        "1. **Scrapes interview experiences** from GeeksforGeeks' company-wise listings  \n",
        "2. **Infers roles** (SDE-1/2/3) using regex-based experience parsing  \n",
        "3. **Summarizes content** using Groq's gemma2-9b-it (per-interview) and deepseek-r1-distill-llama-70b (aggregated) models  \n",
        "4. **Generates PDF reports** with topic distribution charts and clickable question links  \n",
        "5. **Preserves source links** while cleaning HTML artifacts  \n",
        "\n",
        "**Final Outputs**  \n",
        "- Individual interview summaries (CSV)  \n",
        "- Aggregated markdown summary (CSV)  \n",
        "- Coding question reference table (CSV)  \n",
        "- Styled PDF report with visual analytics  \n",
        "\n",
        "This represents the synthesized final product of the user's earlier data processing, summarization, and PDF generation code components, now unified into a single executable pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIgyYXXJYO4O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "import base64\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, Tag, NavigableString\n",
        "from groq import Groq\n",
        "import matplotlib.pyplot as plt\n",
        "from weasyprint import HTML\n",
        "import markdown\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\", \"Groq_api_key\"))\n",
        "BASE_URL = \"https://www.geeksforgeeks.org/interview-experiences/experienced-interview-experiences-company-wise/\"\n",
        "\n",
        "def infer_role_and_years(title):\n",
        "    m = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(?:yr|year)', title, re.IGNORECASE)\n",
        "    yrs = float(m.group(1)) if m else 0.0\n",
        "    if yrs <= 2: return yrs, 'SDE-1'\n",
        "    elif yrs <= 5: return yrs, 'SDE-2'\n",
        "    return yrs, 'SDE-3'\n",
        "\n",
        "def scrape_company_experiences(company):\n",
        "    soup = BeautifulSoup(requests.get(BASE_URL).text, \"lxml\")\n",
        "    label_node = soup.find(string=re.compile(rf'^\\s*{re.escape(company)}\\s*:$'))\n",
        "    if not label_node:\n",
        "        print(f\"‚ùå Could not locate section for '{company}'.\")\n",
        "        return []\n",
        "    entries = []\n",
        "    for elem in label_node.next_elements:\n",
        "        if isinstance(elem, NavigableString) and re.match(r'^\\s*[A-Za-z0-9 &]+\\s*:$', elem.strip()) and elem.strip() != f\"{company}:\":\n",
        "            break\n",
        "        if isinstance(elem, Tag) and elem.name == \"a\" and elem.get(\"href\"):\n",
        "            title = elem.get_text(strip=True)\n",
        "            link = elem[\"href\"]\n",
        "            yrs, role = infer_role_and_years(title)\n",
        "            entries.append({\"Company\": company, \"Title\": title, \"Link\": link, \"Years\": yrs, \"Role\": role})\n",
        "    return entries\n",
        "\n",
        "def fetch_full_text(link):\n",
        "    soup = BeautifulSoup(requests.get(link, headers={'User-Agent': 'Mozilla/5.0'}).text, 'html.parser')\n",
        "    text_div = soup.find('div', class_='text') or soup.find('div', class_='entry-content') or soup.find('article') or soup.body\n",
        "    return str(text_div) if text_div else ''\n",
        "\n",
        "def summarize_single_experience(html_content):\n",
        "    if pd.isnull(html_content): return \"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    clean_text = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are given a software-engineering interview write-up in HTML. Extract and summarize it in 3‚Äì5 bullet points.\n",
        "\n",
        "Cover:\n",
        "- Technical questions asked (by topic: Array, Tree, String, DP, Graph)\n",
        "- System design components\n",
        "- Behavioral questions\n",
        "- Interview structure/rounds\n",
        "\n",
        "Preserve markdown links from `<a>` tags like `[Question](URL)`.\n",
        "\n",
        "---\n",
        "**Text:** {clean_text[:3000]}\n",
        "---\n",
        "**HTML:** {html_content[:3000]}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gemma2-9b-it\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.2,\n",
        "            max_completion_tokens=400\n",
        "        )\n",
        "        return completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error summarizing: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def generate_final_summary(merged_summary,role):\n",
        "    prompt = f\"\"\"\n",
        "You are an expert technical interviewer.\n",
        "\n",
        "Given merged {role.upper()} summaries, generate a detailed report for a prep PDF.\n",
        "\n",
        "Include:\n",
        "- Key topics (Array, Tree, String, DP, Graph, System Design)\n",
        "- Behavioral questions & sample answers\n",
        "- Coding questions with links\n",
        "- System design topics\n",
        "- Interview structure\n",
        "- Common advice/tips\n",
        "\n",
        "Format professionally in markdown.\n",
        "\n",
        "---\n",
        "{merged_summary[:6000]}\n",
        "---\n",
        "\"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"deepseek-r1-distill-llama-70b\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.2,\n",
        "            max_completion_tokens=2048\n",
        "        )\n",
        "        return completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error finalizing summary: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def export_cleaned_pdf(company, role, summary_md, output_path):\n",
        "    # Remove <think>...</think>\n",
        "    summary_md_cleaned = re.sub(r'<think>.*?</think>', '', summary_md, flags=re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    # Analyze topic distribution\n",
        "    topics = ['Array', 'Tree', 'String', 'DP', 'Graph', 'System Design', 'Behavioral Questions']\n",
        "    topic_counts = {\n",
        "        topic: len(re.findall(rf'\\b{topic}\\b', summary_md_cleaned, flags=re.IGNORECASE))\n",
        "        for topic in topics\n",
        "    }\n",
        "    topic_counts = {k: v for k, v in topic_counts.items() if v > 0}\n",
        "\n",
        "    # Pie chart\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    ax.pie(topic_counts.values(), labels=topic_counts.keys(), autopct='%1.1f%%', startangle=140)\n",
        "    ax.set_title(\"Topic Distribution in Interview Summary\")\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png', bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    buf.seek(0)\n",
        "    img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n",
        "    img_html = f'<img src=\"data:image/png;base64,{img_base64}\" alt=\"Topic Distribution Chart\" style=\"max-width:100%; height:auto;\">'\n",
        "\n",
        "    # Convert markdown to HTML\n",
        "    html_summary = markdown.markdown(summary_md_cleaned, extensions=['extra', 'tables', 'sane_lists'])\n",
        "\n",
        "    # Final HTML with styles\n",
        "    styled_html = f\"\"\"\n",
        "    <html>\n",
        "    <head>\n",
        "        <meta charset=\"utf-8\">\n",
        "        <style>\n",
        "            @page {{ size: A4; margin: 1in; }}\n",
        "            body {{\n",
        "                font-family: 'Segoe UI', sans-serif;\n",
        "                font-size: 14px;\n",
        "                color: #333;\n",
        "                line-height: 1.6;\n",
        "            }}\n",
        "            table {{ border-collapse: collapse; width: 100%; }}\n",
        "            th, td {{ border: 1px solid #ccc; padding: 8px; }}\n",
        "            h1, h2, h3 {{ color: #2c3e50; }}\n",
        "            a {{ color: #1e88e5; text-decoration: underline; }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>{company} - {role} Interview Preparation Summary</h1>\n",
        "        <h2>üìä Topic Distribution</h2>\n",
        "        {img_html}\n",
        "        <hr>\n",
        "        {html_summary}\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    HTML(string=styled_html, base_url='.').write_pdf(output_path)\n",
        "    print(f\"‚úÖ PDF created: {output_path}\")\n",
        "\n",
        "def orchestrate_full_pipeline():\n",
        "    company = input(\"Enter Company name: \").strip()\n",
        "    role = input(\"Enter Role (e.g. SDE-1): \").strip()\n",
        "\n",
        "    print(\"üîç Scraping...\")\n",
        "    entries = scrape_company_experiences(company)\n",
        "    if not entries:\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(entries)\n",
        "    df = df[df['Role'].str.upper() == role.upper()]\n",
        "    if df.empty:\n",
        "        print(f\"‚ùå No entries found for {role}\")\n",
        "        return\n",
        "\n",
        "    print(\"üì• Fetching interview content...\")\n",
        "    df['Interview_Experience'] = [fetch_full_text(link) for link in df['Link']]\n",
        "\n",
        "    print(\"üß† Summarizing interviews...\")\n",
        "    df['Interview_Summary'] = [summarize_single_experience(html) for html in df['Interview_Experience']]\n",
        "    df.to_csv(f\"{company.lower()}_{role.lower()}_individual_summaries.csv\", index=False)\n",
        "\n",
        "    print(\"üìÑ Generating final markdown summary...\")\n",
        "    merged_summary = '\\n'.join(df['Interview_Summary'].astype(str))\n",
        "    final_summary = generate_final_summary(merged_summary,role)\n",
        "\n",
        "    # Save to CSV\n",
        "    summary_csv_path = f\"{company.lower()}_{role.lower()}_final_summary.csv\"\n",
        "    pd.DataFrame({'Summary': [final_summary]}).to_csv(summary_csv_path, index=False)\n",
        "    print(f\"üìù Summary saved to {summary_csv_path}\")\n",
        "\n",
        "    # Extract coding question links\n",
        "    print(\"üîó Extracting coding links...\")\n",
        "    matches = re.findall(r'\\[([^\\]]+)\\]\\((https?://[^\\)]+)\\)', final_summary)\n",
        "    pd.DataFrame(matches, columns=['Question', 'Link']).to_csv(f\"{company.lower()}_coding_questions.csv\", index=False)\n",
        "\n",
        "    # PDF with styled HTML and chart\n",
        "    print(\"üìÑ Building PDF...\")\n",
        "    export_cleaned_pdf(company, role, final_summary, f\"{company.lower()}_{role.lower()}_summary.pdf\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    orchestrate_full_pipeline()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matplotlib is building the font cache; this may take a moment.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "from weasyprint import HTML\n",
        "import markdown\n",
        "\n",
        "def export_cleaned_pdf(company, role, summary_md, output_path):\n",
        "    # Remove <think>...</think> blocks\n",
        "    summary_md_cleaned = re.sub(r'<think>.*?</think>', '', summary_md, flags=re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    # Analyze topic distribution\n",
        "    topics = ['Array', 'Tree', 'String', 'DP', 'Graph', 'System Design', 'Behavioral Questions']\n",
        "    topic_counts = {\n",
        "        topic: len(re.findall(rf'\\b{topic}\\b', summary_md_cleaned, flags=re.IGNORECASE))\n",
        "        for topic in topics\n",
        "    }\n",
        "    topic_counts = {k: v for k, v in topic_counts.items() if v > 0}\n",
        "\n",
        "    # Pie chart of topics\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    ax.pie(topic_counts.values(), labels=topic_counts.keys(), autopct='%1.1f%%', startangle=140)\n",
        "    ax.set_title(\"Topic Distribution in Interview Summary\")\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png', bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    buf.seek(0)\n",
        "    img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n",
        "    img_html = f'<img src=\"data:image/png;base64,{img_base64}\" alt=\"Topic Distribution Chart\" style=\"max-width:100%; height:auto;\">'\n",
        "\n",
        "    # Convert markdown to HTML\n",
        "    html_summary = markdown.markdown(summary_md_cleaned, extensions=['extra', 'tables', 'sane_lists'])\n",
        "\n",
        "    # Final styled HTML for PDF\n",
        "    styled_html = f\"\"\"\n",
        "    <html>\n",
        "    <head>\n",
        "        <meta charset=\"utf-8\">\n",
        "        <style>\n",
        "            @page {{ size: A4; margin: 1in; }}\n",
        "            body {{\n",
        "                font-family: 'Segoe UI', sans-serif;\n",
        "                font-size: 14px;\n",
        "                color: #333;\n",
        "                line-height: 1.6;\n",
        "            }}\n",
        "            table {{ border-collapse: collapse; width: 100%; }}\n",
        "            th, td {{ border: 1px solid #ccc; padding: 8px; }}\n",
        "            h1, h2, h3 {{ color: #2c3e50; }}\n",
        "            a {{ color: #1e88e5; text-decoration: underline; }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>{company} - {role} Interview Preparation Summary</h1>\n",
        "        <h2>üìä Topic Distribution</h2>\n",
        "        {img_html}\n",
        "        <hr>\n",
        "        {html_summary}\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    HTML(string=styled_html, base_url='.').write_pdf(output_path)\n",
        "    print(f\"‚úÖ PDF created: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_md = "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
